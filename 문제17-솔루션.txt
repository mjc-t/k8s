1. XShell, XFTP 다운로드
2. 디스크 관리자: 첫번째 하드디스크의 뒷부분에 단순볼륨 생성(D드라이브)
* VMWare 네트워크 에디터: 10.0.5.0/24
3. XFTP 이용 -> 10.11.82.61 접속해서 모두 다운로드-> D드라이브
4. github.com/mjc-t/k8s에서 파일 3개 다운로드
5. cka 파일 복사 -> 원본을 따로 둘 것

console 주소: 10.0.2.5
ubuntu:ubuntu, root:password

kubectl config use-context 컨텍스트명
kubectl config current-context
kubectl get node
sudo apt update
sudo apt install kubectx 
kubectx k8s

문제 1.
1. custom-app 존재 여부 확인
	kubectl get pod custom-app
2. 로그 확인
	kubectl logs custom-app | grep "error"
3. 리다이렉션 > /var/CKA2022/errors.txt
	kubectl logs custom-app | grep "error" > /var/CKA2022/errors.txt
4. 파일 내용 확인
	cat /var/CKA2022/errors.txt

문제 5 클러스터 롤
1. 네임스페이스
	kubectl create ns app-team1
2. 서비스 어카운트
	kubectl create sa cicd-token -n app-team1
3. 클러스터 롤
	kubectl create clusterrole deployment-clusterrole \
	--resource=deployment,statefulset,daemonset --verb=create
4. 클러스터 롤 바인딩
	kubectl create clusterrolebinding \
	deployment-clusterrolebinding \
	--clusterrole=deployment-clusterrole \
	--serviceaccount=app-team1:cicd-token -n app-team1
	
문제6. 파드 생성
1. 검색: command
2. 네임스페이스
	kubectl create ns cka-exam
3. 파드용 야물 생성
	kubectl run 파드명 --image=이미지명 -n 네임스페이스 \
		-o yaml --dry-run=clinet > 야물파일명
		
	kubectl run pod-01 --image=busybox \
	--env=CERT="CKA-cert" -n cka-exam \
	-o yaml --dry-run=client > pod-01.yaml
4.      //   수정: env, command, args
	vi pod-01.yaml
5.      //   적용
	kubectl apply -f pod-01.yaml


야물파일로도 삭제 가능
kubectl delete -f 야물파일명
---
문제 9
1. 네임스페이스 devops의 eshop-order 디플로이먼트 확인
	kubectl get deploy eshop-order -n devops
2. kubectl scale deploy 디플로이먼트명 -n devops --replicas=5
	kubectl scale deploy eshop-order -n devops --replicas=5
3. kubectl get deploy 디플로이먼트명 -n devops
	kubectl get deploy eshop-order -n devops
	kubectl get po -n devops
---
문제  10
1. nginx-app 디플로이먼트 생성
	kubectl create deploy nginx-app \
	--image=nginx:1.11.10-alpine --replicas=3
2. 디플로이먼트, 파드 확인
	kubectl get po | grep nginx-app
	kubectl describe pod 파드명 | grep -i image
3. kubectl set image deploy 디플로이먼트명 컨테이너명=이미지명
	kubectl set image deploy nginx-app nginx=nginx:1.11.13-alpine
4. 디플로이먼트, 파드 확인
	kubectl get po | grep nginx-app
	kubectl describe pod 파드명 | grep -i image
5. 롤백: kubectl rollout undo deploy 디플로이먼트명
	kubectl rollout undo deploy nginx-app
6. 디플로이먼트, 파드 확인
	kubectl get po | grep nginx-app
	kubectl describe pod 파드명 | grep -i image
---
문제 11
1. 검색: netpol
2. vi netpol.yaml(name, namespace, match labels, port)
다른 창
2-1. kubectl get ns --show-labels
	kubectl get pod -n devops --show-labels
	kubectl get ns migops --show-labels
3. kubectl apply -f netpol.yaml
4. kubectl get netpol -n devops
---
문제 12
1. 검색: pv(kind: pershostpath)
2. vi pv.yaml(name, storage, accessmode+hostpath)
3. kubectl apply -f pv.yaml
* kubectl get pv 
---
문제 13
1. 디플로이먼트 front-end 확인
	kubectl get deploy front-end
2. kubectl expose를 통해 서비스 생성
	kubectl expose deploy front-end --name=front-end-svc \
	--type=NodePort --port=80 --target-port=80
* kubectl get svc front-end-svc
===
만약 포트번호가 제시되어 있다면
1. 디플로이먼트 front-end 확인
	kubectl get deploy front-end
2. kubectl expose를 통해 서비스용 야물파일 생성
	kubectl expose deploy front-end --name=front-end-svc \
	--type=NodePort --port=80 --target-port=80 \
	--dry-run=client -o yaml > front-end-svc.yaml
3. vi front-end-svc.yaml(nodePort 추가)
4. front-end-svc.yaml 적용
	kubectl apply -f front-end-svc.yaml
* kubectl get svc front-end-svc
---
문제 14
1. 파드 nginx-resolver 생성
	kubectl run nginx-resolver --image=nginx --port=80
* 파드 생성 확인
	kubectl get pod nginx-resolver -o wide
2. 서비스 nginx-resolver-service 생성
	kubectl expose pod nginx-resolver \
	--name=nginx-resolver-service --port=80
* 서비스 생성 확인
	kubectl get svc nginx-resolver-service
3. Lookup용 임시파드 생성 및 서비스 lookup
	kubectl run test-nslookup --image=busybox:1.28 \
	-it --restart=Never --rm -- nslookup 서비스주소
4.                    //             > /var/CKA2022/nginx.svc
* cat /var/CKA2022/nginx.svc
5.                    //   파드주소 lookup
	kubectl run test-nslookup --image=busybox:1.28 \
	-it --restart=Never --rm \
	-- nslookup 10-44-0-15.default.pod.cluster.local
6.                    //             > /var/CKA2022/nginx.pod
* cat /var/CKA2022/nginx.pod
---
문제 15
1. 검색: pv(cloning)
2. pvc.yaml 생성 및 수정(name, class, storage)
	vi pvc.yaml
3. pvc.yaml 적용
	kubectl apply -f pvc.yaml
* pvc 확인
4. 검색: pv(kind: Pod-2번째)
5. pvc-pod.yaml 생성 및 수정(name, mountpath, claimname)
	vi pvc-pod.yaml
6.     //   적용
	kubectl apply -f pvc-pod.yaml
* 파드 확인
	kubectl get pod web-server
	kubectl describe pod web-server | grep -i pv-volume
---
문제 17
1. 노드 확인
	kubectl get node
2. 파드 확인
	kubectl get pod -A
3. 드레인
	kubectl drain k8s-worker1
	           //            ...
	kubectl drain k8s-worker1 \
	--delete-emptydir-data --force --ignore-daemonsets
* 확인
	kubectl get no
	kubectl get po -A -o wide
4. 언코든
	kubectl uncordon k8s-worker1
* 확인
	kubectl get no
---
문제 4
1. 검색: etcd backup(volume snapshot)
2. ssh k8s-master
3. sudo -i
4. 또다른 창: nano로 수정 후 붙여넣기(잘 안되면 역슬래시 제거)
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
 --cert=/etc/kubernetes/pki/etcd/server.crt \
 --key=/etc/kubernetes/pki/etcd/server.key \
 snapshot save /data/etcdsnapshot.db
5. ls /data/etcdsnapshot.db
6. 
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
 --cert=/etc/kubernetes/pki/etcd/server.crt \
 --key=/etc/kubernetes/pki/etcd/server.key \
 snapshot restore /data/etcd-snapshot-previous.db
8. tree default.etcd/member
9. exit -> exit (root > master > console로 복귀)
---
문제 16. 
1. 검색: kubeadm upgrade
2. kubectl get node3. 
3. ssh k8s-master
4. sudo -i
5. 순서대로
6. 또다른 창: vi (버전 변경)
sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.32.7-1.1' && \
sudo apt-mark hold kubeadm
7. 순서대로
8. vi (버전 변경)
sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.32.7-1.1' kubectl='1.32.7-1.1' && \
sudo apt-mark hold kubelet kubectl
9. 순서대로
10. exit > exit
11. kubectl get node
---
문제 8
1. 검색: sidecar -> logging architecture 선택
2. 그림 3번째 후 박스 2번째
3. eshop-cart-app 파드 확인
	kubectl get po eshop-cart-app
4. 야물파일 생성
	kubectl get po eshop-cart-app -o yaml > sidecar.yaml
5. 야물파일 수정(volumes 포함 3줄 그대로, 6줄 복사,
	name, image, args 수정) 
6. eshop-cart-app 파드 삭제 (--force)
	kubectl delete po eshop-cart-app --force
7. 야물파일 적용
	kubectl apply -f sidecar.yaml
8. 확인
	kubectl get po eshop-cart-app
---
문제 2
1. 노드 확인
	kubectl get no
2. ssh hk8s-worker2
3. sudo -i
4. systemctl status docker
*  systemctl status containerd
5. systemctl status kubelet
6. systemctl enable --now kubelet	
=  systemctl start kubelet, systemctl enable kubelet
7. systemctl status kubelet
8. exit > exit
9. kubectl get no
---
문제 3
1. kubectl get no | grep -iw ready | wc -l 
2. kubectl get no | grep -iw ready | wc -l > /var/CKA2022/count.txt
---
문제 7
1. 야물파일 생성
	kubectl run eshop-frontend --image=nginx \
	--dry-run=client -o yaml > multi-pod.yaml
2. 야물파일 수정
	vi multi-pod.yaml
3. 야물파일 적용
	kubectl apply -f multi-pod.yaml
* 확인
	kubectl get po eshop-frontend

















































	
	
	
	
